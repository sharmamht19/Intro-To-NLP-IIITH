{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "312853fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05aa987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramNegativeSampling, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.embeddings.weight.size(1)\n",
    "        self.embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embeddings.weight.data.uniform_(-0, 0)\n",
    "\n",
    "    def forward(self, center_words, target_words, neg_words):\n",
    "        # Convert inputs to tensors if not already tensors\n",
    "        if not isinstance(center_words, torch.Tensor):\n",
    "            center_words = torch.tensor(center_words)\n",
    "        if not isinstance(target_words, torch.Tensor):\n",
    "            target_words = torch.tensor(target_words)\n",
    "        \n",
    "        embeds = self.embeddings(center_words)\n",
    "        out_embeds = self.out_embeddings(target_words)\n",
    "        neg_embeds = -self.out_embeddings(neg_words)\n",
    "        \n",
    "        score = torch.sum(torch.mul(embeds, out_embeds), dim=1)\n",
    "        score = torch.sigmoid(score)\n",
    "        \n",
    "        neg_score = torch.bmm(neg_embeds, embeds.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.sum(torch.log(torch.sigmoid(neg_score)), dim=1)\n",
    "        \n",
    "        return -torch.mean(torch.log(score) + neg_score)\n",
    "\n",
    "class SkipGramWord2Vec:\n",
    "    def __init__(self, embedding_dim=100, num_neg_samples=4, min_count=2, batch_size=128, learning_rate=0.001, num_epochs=5):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_neg_samples = num_neg_samples\n",
    "        self.min_count = min_count\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.word_counts = {}\n",
    "        self.word_freqs = []\n",
    "        self.model = None\n",
    "        self.unk_token = '<unk>'\n",
    "        self.unk_index = None  # Index for the <unk> token\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def load_dataset(self, file_path, num_samples=10000, reduce_label=True):\n",
    "        if num_samples:\n",
    "            data = pd.read_csv(file_path, nrows=num_samples)\n",
    "        else:\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "        if reduce_label and 'Class Index' in data.columns:\n",
    "            data['Class Index'] -= 1  # Reduce 1 from each label\n",
    "\n",
    "        corpus = data['Description']\n",
    "        return corpus\n",
    "\n",
    "  \n",
    "    def preprocess_data(self, corpus):\n",
    "        cleaned_sentences = []\n",
    "        for sentence in corpus:\n",
    "            # Tokenize sentence using regular expression\n",
    "            tokens = re.findall(r'\\b\\w+\\b', sentence.lower())  # This regex tokenizes words, removing punctuation\n",
    "            cleaned_sentences.append(tokens)\n",
    "        return cleaned_sentences\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        word_counts = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        word_counts = {word: count for word, count in word_counts.items() if count >= self.min_count}\n",
    "        word_counts[self.unk_token] = 0\n",
    "        sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.word_to_idx = {word: idx + 1 for idx, (word, _) in enumerate(sorted_word_counts)}  # Shift indices by 1 to accommodate the <unk> token\n",
    "        self.word_to_idx[self.unk_token] = 0  # Assign index 0 to the <unk> token\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.word_counts = {word: count for word, count in sorted_word_counts}\n",
    "        total_words = sum(word_counts.values())\n",
    "        self.word_freqs = np.array([count / total_words for count in word_counts.values()])\n",
    "\n",
    "        # Assign index to <unk> token\n",
    "        self.unk_index = 0\n",
    "        \n",
    "        \n",
    "    def build_data(self, sentences, window_size = 2):\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            for i, center_word in enumerate(sentence):\n",
    "                for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                    if i != j:\n",
    "                        data.append((center_word, sentence[j]))\n",
    "        return data\n",
    "\n",
    "    def get_neg_sample(self, target_word_idx, num_samples):\n",
    "        neg_samples = []\n",
    "        while len(neg_samples) < num_samples:\n",
    "            sample = np.random.choice(len(self.word_to_idx), size=num_samples, p=self.word_freqs)\n",
    "            if target_word_idx not in sample:\n",
    "                neg_samples.extend(sample)\n",
    "        return torch.LongTensor(neg_samples)\n",
    "    \n",
    "    def save_word_vectors(self, file_path):\n",
    "        word_to_idx = self.word_to_idx\n",
    "        idx_to_word = self.idx_to_word\n",
    "        word_vectors = self.model.embeddings.weight.cpu().detach().numpy()\n",
    "\n",
    "        state_dict = {\n",
    "            'word_to_idx': word_to_idx,\n",
    "            'idx_to_word': idx_to_word,\n",
    "            'word_vectors': word_vectors\n",
    "        }\n",
    "\n",
    "        torch.save(state_dict, file_path)\n",
    "        print(f\"Word vectors and vocabulary mappings saved to {file_path}.\") \n",
    "\n",
    "    def train(self, sentences, window_size=2):\n",
    "        self.build_vocab(sentences)\n",
    "        data = self.build_data(sentences, window_size)\n",
    "        total_batches = len(data) // self.batch_size\n",
    "        print(\"Device: \", self.device)\n",
    "        self.model = SkipGramNegativeSampling(len(self.word_to_idx), self.embedding_dim).to(self.device)\n",
    "            \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            total_loss = 0.0\n",
    "            with tqdm(total=total_batches, desc=f'Epoch {epoch+1}/{self.num_epochs}', unit='batch') as pbar:\n",
    "                for i in range(0, len(data), self.batch_size):\n",
    "                    batch_data = data[i:i+self.batch_size]\n",
    "                    center_words, target_words = zip(*batch_data)\n",
    "\n",
    "                    # Convert words to indices\n",
    "                    center_word_idxs = [self.word_to_idx.get(word, 0) for word in center_words]  \n",
    "                    target_word_idxs = [self.word_to_idx.get(word, 0) for word in target_words]  \n",
    "\n",
    "                    center_words_tensor = torch.LongTensor(center_word_idxs).to(self.device)\n",
    "                    target_words_tensor = torch.LongTensor(target_word_idxs).to(self.device)\n",
    "\n",
    "                    neg_words = [self.get_neg_sample(target_word_idx, self.num_neg_samples) for target_word_idx in target_word_idxs]\n",
    "\n",
    "                    # Convert neg_words to tensor\n",
    "                    neg_words_tensor = torch.stack(neg_words).to(self.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.model(center_words_tensor, target_words_tensor, neg_words_tensor)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    pbar.set_postfix({'loss': total_loss / ((i // self.batch_size) + 1)})\n",
    "                    pbar.update()\n",
    "                    \n",
    "         # Save word vectors and vocabulary mappings\n",
    "        self.save_word_vectors('skip-gram-word-vectors.pt')\n",
    "        \n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def get_word_vector(self, word):\n",
    "        if word in self.word_to_idx:\n",
    "            idx = torch.tensor([self.word_to_idx[word]]).to(self.device)\n",
    "            return self.model.embeddings(idx).squeeze().cpu().detach().numpy()\n",
    "        else:\n",
    "            print(f\"Word '{word}' not found in vocabulary. Using <unk> token.\")\n",
    "            idx = torch.tensor([self.unk_index]).to(self.device)\n",
    "            return self.model.embeddings(idx).squeeze().cpu().detach().numpy()\n",
    "\n",
    "    def most_similar(self, word, topn=5):\n",
    "        if word in self.word_to_idx:\n",
    "            word_vec = self.get_word_vector(word)\n",
    "            if word_vec is not None:\n",
    "                all_word_vecs = self.model.embeddings.weight.cpu().detach().numpy()\n",
    "                similarities = np.dot(all_word_vecs, word_vec)\n",
    "                top_similar_words = np.argsort(-similarities)[1:topn+1]\n",
    "                return [(self.idx_to_word[idx], similarities[idx]) for idx in top_similar_words]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "986138ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 14872batch [10:18, 24.03batch/s, loss=1.7]                           \n",
      "Epoch 2/4: 14872batch [10:35, 23.39batch/s, loss=1.46]                          \n",
      "Epoch 3/4: 14872batch [10:21, 23.92batch/s, loss=1.38]                          \n",
      "Epoch 4/4: 14872batch [10:01, 24.73batch/s, loss=1.31]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors and vocabulary mappings saved to skip-gram-word-vectors.pt.\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word2vec_model = SkipGramWord2Vec(embedding_dim=100, num_neg_samples=4, num_epochs=4)\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"data/train.csv\"\n",
    "corpus = word2vec_model.load_dataset(file_path, num_samples=15000)\n",
    "sentences = word2vec_model.preprocess_data(corpus)\n",
    "\n",
    "word2vec_model.train(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = word2vec_model.get_word_vector(\"I\")\n",
    "print(\"Word vector for 'word':\", word_vector)\n",
    "\n",
    "similar_words = word2vec_model.most_similar(\"I\")\n",
    "print(\"Most similar words to 'word':\", similar_words)\n",
    "print( word2vec_model.model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "1672b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.recurrent_layer = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc_layer = nn.Linear( hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Forward pass through the RNN\n",
    "        recurrent_output, _ = self.recurrent_layer(input_seq)\n",
    "        \n",
    "        # Extract the last hidden state\n",
    "        last_hidden_state = recurrent_output[:, -1, :]\n",
    "        \n",
    "        # Pass the last hidden state through the fully connected layer\n",
    "        logits = self.fc_layer(last_hidden_state)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def predict(self, logits):\n",
    "        \n",
    "        # Apply argmax to get the index of the highest logit\n",
    "        predicted_class_index = torch.argmax(logits).item()\n",
    "        \n",
    "        return predicted_class_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "202f2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs=10, lr=0.001):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.model = Model(input_size, hidden_size, output_size).to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, train_data, model_save_path):\n",
    "        self.model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_accuracies = []\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            running_loss = 0.0\n",
    "            train_data_with_progress = tqdm(train_data, desc=f'Epoch {epoch+1}/{self.num_epochs}')\n",
    "            predictions = []\n",
    "            labels_list = []\n",
    "            for inputs, label in train_data_with_progress:\n",
    "                inputs, label = inputs.to(self.device), label.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, label)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                predicted_label = self.model.predict(outputs)\n",
    "                predictions.append(predicted_label)\n",
    "                labels_list.append(label.item())  \n",
    "#             prediction_counts = Counter(predictions)\n",
    "\n",
    "#             # Print the count of each unique number\n",
    "#             for label, count in prediction_counts.items():\n",
    "#                 print(f\"Label {label}: {count} occurrences\")\n",
    "            avg_loss = running_loss / len(labels_list)\n",
    "            accuracy = sum(p == l for p, l in zip(predictions, labels_list)) / len(predictions)\n",
    "\n",
    "            epoch_losses.append(avg_loss)\n",
    "            epoch_accuracies.append(accuracy)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{self.num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "            \n",
    "        self.save_model(model_save_path)\n",
    "        \n",
    "        return epoch_losses, epoch_accuracies\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        self.model.eval()  \n",
    "        predictions = []\n",
    "        labels_list = []\n",
    "        with torch.no_grad():  \n",
    "            for inputs, label in test_data:\n",
    "                inputs, label = inputs.to(self.device), label.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                predicted_label = self.model.predict(outputs)\n",
    "                predictions.append(predicted_label)\n",
    "                labels_list.append(label.item())\n",
    "        \n",
    "#         prediction_counts = Counter(predictions)\n",
    "\n",
    "#         # Print the count of each unique number\n",
    "#         for label, count in prediction_counts.items():\n",
    "#             print(f\"Label {label}: {count} occurrences\")\n",
    "            \n",
    "        accuracy = sum(p == l for p, l in zip(predictions, labels_list)) / len(predictions)\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_metrics(self, data_loader):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                predicted_label = self.model.predict(outputs)\n",
    "                predictions.append(predicted_label)\n",
    "                labels_list.append(labels.item())\n",
    "\n",
    "        accuracy = accuracy_score(labels_list, predictions)\n",
    "        precision = precision_score(labels_list, predictions, average='macro')\n",
    "        recall = recall_score(labels_list, predictions, average='macro')\n",
    "        f1 = f1_score(labels_list, predictions, average='macro')\n",
    "        confusion_mat = confusion_matrix(labels_list, predictions)\n",
    "\n",
    "        return accuracy, precision, recall, f1, confusion_mat\n",
    "\n",
    "    def evaluate_performance(self, train_loader, test_loader):\n",
    "        print(\"Evaluation on Train Set:\")\n",
    "        train_accuracy, train_precision, train_recall, train_f1, train_confusion_mat = self.calculate_metrics(train_loader)\n",
    "        print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Train Precision: {train_precision:.4f}\")\n",
    "        print(f\"Train Recall: {train_recall:.4f}\")\n",
    "        print(f\"Train F1 Score: {train_f1:.4f}\")\n",
    "        print(\"Train Confusion Matrix:\")\n",
    "        print(train_confusion_mat)\n",
    "\n",
    "        print(\"\\nEvaluation on Test Set:\")\n",
    "        test_accuracy, test_precision, test_recall, test_f1, test_confusion_mat = self.calculate_metrics(test_loader)\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"Test Recall: {test_recall:.4f}\")\n",
    "        print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "        print(\"Test Confusion Matrix:\")\n",
    "        print(test_confusion_mat)\n",
    "        \n",
    "    def save_model(self, filepath):\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "        print(f'Model saved to {filepath}')\n",
    "\n",
    "    def predict_class(self, sentence):\n",
    "        input_tensor = sentence.to(self.device)\n",
    "        self.model.eval()  \n",
    "        with torch.no_grad():  \n",
    "            output = self.model(input_tensor)\n",
    "            predicted_label = self.model.predict(output)\n",
    "        \n",
    "        return predicted_label + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "368b9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipGramTextDataProcessor:\n",
    "    def __init__(self, word_to_idx=None, idx_to_word=None, word_vectors=None):\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_vectors = word_vectors\n",
    "\n",
    "    def load_dataset(self, file_path, num_samples=None, reduce_label=True):\n",
    "        if num_samples:\n",
    "            data = pd.read_csv(file_path, nrows=num_samples)\n",
    "        else:\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "        if reduce_label and 'Class Index' in data.columns:\n",
    "            data['Class Index'] -= 1  # Reduce 1 from each label\n",
    "\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data, text_column='Description', label_column='Class Index'):\n",
    "        corpus = data[text_column]\n",
    "        labels = data[label_column] if label_column in data.columns else None\n",
    "        return corpus, labels\n",
    "\n",
    "    def load_word_vectors(self, file_path):\n",
    "        state_dict = torch.load(file_path)\n",
    "        self.word_to_idx = state_dict['word_to_idx']\n",
    "        self.idx_to_word = state_dict['idx_to_word']\n",
    "        self.word_vectors = state_dict['word_vectors']\n",
    "\n",
    "    def get_word_embedding(self, word):\n",
    "        if self.word_to_idx is not None and self.word_vectors is not None:\n",
    "            if word in self.word_to_idx:\n",
    "                idx = self.word_to_idx[word]\n",
    "                return self.word_vectors[idx]\n",
    "            else:\n",
    "                # If word not found, return zero vector or handle as per your requirement\n",
    "                return self.word_vectors[0]\n",
    "        else:\n",
    "            raise ValueError(\"Word vectors and vocabulary mappings are not loaded.\")\n",
    "            \n",
    "    def get_sentece_embeddings(self,sentence):\n",
    "        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())  \n",
    "        sentence_embeddings = []\n",
    "        for word in tokens:\n",
    "            word_embedding = self.get_word_embedding(word)\n",
    "            sentence_embeddings.append(word_embedding)\n",
    "        sentence_embeddings = torch.tensor(np.array(sentence_embeddings))\n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def preprocess_dataset(self, data, batch_size = 1):\n",
    "        corpus, labels = self.preprocess_data(data)\n",
    "        dataset = []\n",
    "        for sentence in corpus:\n",
    "            sentence_embeddings = self.get_sentece_embeddings(sentence)\n",
    "            dataset.append(sentence_embeddings)\n",
    "        \n",
    "        labels_tensor = torch.tensor(labels)\n",
    "        combined_data = list(zip(dataset, labels_tensor))\n",
    "\n",
    "        train_dataloader = DataLoader(combined_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        return train_dataloader\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "6ce10d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████████████████| 120000/120000 [03:30<00:00, 571.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9005, Accuracy: 0.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████████████████| 120000/120000 [04:01<00:00, 497.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.8825, Accuracy: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████████████████| 120000/120000 [04:56<00:00, 405.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.9014, Accuracy: 0.6358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████████████████| 120000/120000 [05:09<00:00, 387.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.9923, Accuracy: 0.5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████████████████| 120000/120000 [05:03<00:00, 395.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.9746, Accuracy: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████████████████| 120000/120000 [04:51<00:00, 411.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.1425, Accuracy: 0.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████████████████| 120000/120000 [04:32<00:00, 439.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.9860, Accuracy: 0.5784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████████████████| 120000/120000 [04:34<00:00, 437.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.8524, Accuracy: 0.6611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████████████████| 120000/120000 [04:34<00:00, 437.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.0329, Accuracy: 0.5587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|█████████████████████| 120000/120000 [04:34<00:00, 437.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.1270, Accuracy: 0.5073\n",
      "Model saved to skip-gram-classification-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = 100  \n",
    "hidden_size = 128  \n",
    "output_size = 4  \n",
    "batch_size = 1\n",
    "data_processor = skipGramTextDataProcessor()\n",
    "data_processor.load_word_vectors('skip-gram-word-vectors.pt')\n",
    "train_data = data_processor.load_dataset('data/train.csv', num_samples=None)\n",
    "test_data = data_processor.load_dataset('data/test.csv')\n",
    "train_dataset = data_processor.preprocess_dataset(train_data)\n",
    "test_dataset = data_processor.preprocess_dataset(test_data)\n",
    "# print(len(train_data))\n",
    "skip_gram_classifier = Classifier(input_size, hidden_size, output_size, num_epochs=10, lr=0.0005)\n",
    "skip_gram_losses, skip_gram_accuracies = skip_gram_classifier.train(train_dataset,'skip-gram-classification-model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "35834bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5600\n",
      "Evaluation on Train Set:\n",
      "Train Accuracy: 0.5639\n",
      "Train Precision: 0.5744\n",
      "Train Recall: 0.5639\n",
      "Train F1 Score: 0.5515\n",
      "Train Confusion Matrix:\n",
      "[[23635  2757  2479  1129]\n",
      " [ 6290 20482  1206  2022]\n",
      " [11774  2775 10618  4833]\n",
      " [ 5610  6307  5150 12933]]\n",
      "\n",
      "Evaluation on Test Set:\n",
      "Test Accuracy: 0.5600\n",
      "Test Precision: 0.5673\n",
      "Test Recall: 0.5600\n",
      "Test F1 Score: 0.5460\n",
      "Test Confusion Matrix:\n",
      "[[1499  172  161   68]\n",
      " [ 389 1313   72  126]\n",
      " [ 752  186  626  336]\n",
      " [ 366  377  339  818]]\n"
     ]
    }
   ],
   "source": [
    "test_acc = skip_gram_classifier.evaluate(test_dataset)\n",
    "skip_gram_classifier.evaluate_performance(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "54fb5418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 2\n",
      "Losses for each epoch:\n",
      "Epoch 1: 0.9005\n",
      "Epoch 2: 0.8825\n",
      "Epoch 3: 0.9014\n",
      "Epoch 4: 0.9923\n",
      "Epoch 5: 0.9746\n",
      "Epoch 6: 1.1425\n",
      "Epoch 7: 0.9860\n",
      "Epoch 8: 0.8524\n",
      "Epoch 9: 1.0329\n",
      "Epoch 10: 1.1270\n",
      "\n",
      "Accuracies for each epoch:\n",
      "Epoch 1: 0.6373\n",
      "Epoch 2: 0.6500\n",
      "Epoch 3: 0.6358\n",
      "Epoch 4: 0.5609\n",
      "Epoch 5: 0.5882\n",
      "Epoch 6: 0.4975\n",
      "Epoch 7: 0.5784\n",
      "Epoch 8: 0.6611\n",
      "Epoch 9: 0.5587\n",
      "Epoch 10: 0.5073\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Thisksndkf issjkngkasd predicteskfmkad class for this sentencealjsndjfkna jansdfkn ksjdfk.\"\n",
    "\n",
    "embeddings = data_processor.get_sentece_embeddings(sentence).unsqueeze(0)\n",
    "# print(embeddings)\n",
    "predicted_class = skip_gram_classifier.predict_class(embeddings)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "print(\"Losses for each epoch:\")\n",
    "for epoch, loss in enumerate(skip_gram_losses):\n",
    "    print(f\"Epoch {epoch+1}: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nAccuracies for each epoch:\")\n",
    "for epoch, accuracy in enumerate(skip_gram_accuracies):\n",
    "    print(f\"Epoch {epoch+1}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0125bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4f4df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
