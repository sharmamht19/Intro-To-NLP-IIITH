{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "109a59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing \n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b3a98ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer:\n",
    "    def __init__(self):\n",
    "        self.mailid_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        self.url_regex = r'https?://\\S+|www\\.\\S+'\n",
    "        self.mention_regex = r'@\\w+'\n",
    "        self.hashtag_regex = r'#\\w+'\n",
    "        self.number_regex = r'\\b\\d+\\b'\n",
    "        self.word_regex = r'\\w+'\n",
    "#         self.punctuation_regex = r'[^\\w\\s]'\n",
    "        self.sentence_regex = r'(?<=\\.|\\?|\\s)\\s+'\n",
    "\n",
    "    def split_sentences(self, text):\n",
    "        sentenced_text = re.split(self.sentence_regex, text)\n",
    "        return sentenced_text\n",
    "\n",
    "    def sentence_tokenizer(self, sentence):\n",
    "        # Replacing mails\n",
    "        sentence = re.sub(self.mailid_regex, \"<MAILID>\", sentence)\n",
    "\n",
    "        # Replacing URLs\n",
    "        sentence = re.sub(self.url_regex, \"<URL>\", sentence)\n",
    "\n",
    "        # Replacing mentions\n",
    "        sentence = re.sub(self.mention_regex, \"<MENTION>\", sentence)\n",
    "\n",
    "        # Replacing hashtags\n",
    "        sentence = re.sub(self.hashtag_regex, \"<HASHTAG>\", sentence)\n",
    "\n",
    "        # Replacing numbers\n",
    "        sentence = re.sub(self.number_regex, \"<NUM>\", sentence)\n",
    "#         print(sentence)\n",
    "#         Word tokenizer\n",
    "        words = re.findall(self.word_regex, sentence)\n",
    "#         print(words)\n",
    "        # Append End of sentence and start of sentence to the list of words\n",
    "        tokenized_sentence = [\"<sos>\"] + words + [\"<eos>\"]  \n",
    "        \n",
    "        \n",
    "#         # Word tokenizer including punctuation\n",
    "#         words_with_punctuation = re.findall(r'\\S+|\\s+', sentence)\n",
    "\n",
    "\n",
    "#         # Remove leading and trailing whitespaces\n",
    "#         words_with_punctuation = [word.strip() for word in words_with_punctuation]\n",
    "\n",
    "#         # Append End of sentence and start of sentence to the list of words\n",
    "#         tokenized_sentence = [\"<sos>\"] + words_with_punctuation + [\"<eos>\"]\n",
    "\n",
    "        return tokenized_sentence\n",
    "\n",
    "    def tokenizer(self, text):\n",
    "        # Split the given text into sentences.\n",
    "        sentenced_text = self.split_sentences(text)\n",
    "#         print(sentenced_text)\n",
    "        # Final tokenized tokens.\n",
    "        tokenized_sentences = []\n",
    "\n",
    "        for sentence in sentenced_text:\n",
    "            tokenized_sentences.append(self.sentence_tokenizer(sentence))\n",
    "\n",
    "        return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fe55a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildNGram(n: int, txt: str, tokenizer):\n",
    "    nGramContext = {}  # dictionaries to store N-gram context\n",
    "    nGramCounter = {}  # dictionaries to store N-gram count\n",
    "\n",
    "    # Tokenizing the input text\n",
    "    sentences = tokenizer.tokenizer(txt)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_lower = [word.lower() for word in sentence]\n",
    "#         print(n)\n",
    "        if( n - 2 > 0 ):\n",
    "            sentence = [\"<sos>\"] * (n - 2) + sentence_lower\n",
    "        else:\n",
    "            sentence = sentence_lower\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        tokens = sentence\n",
    "#         print(tokens)\n",
    "        # Iterate through N-grams in the sentence\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            context = \" \".join(tokens[i:i + n - 1])\n",
    "            target_word = tokens[i + n - 1]\n",
    "            total_ngram = context + \" \" + target_word if context else target_word\n",
    "\n",
    "            # Update N-gram counters\n",
    "            if total_ngram in nGramCounter:\n",
    "                nGramCounter[total_ngram] += 1\n",
    "            else:\n",
    "                nGramCounter[total_ngram] = 1\n",
    "\n",
    "                # Update N-gram context\n",
    "                if context in nGramContext:\n",
    "                    nGramContext[context].append(target_word)\n",
    "                else:\n",
    "                    nGramContext[context] = [target_word]\n",
    "\n",
    "    return nGramContext, nGramCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9e413127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodTuringSmoothing:\n",
    "    def __init__(self, ngram_counter):\n",
    "        self.ngram_counter = ngram_counter\n",
    "        self.total_ngrams = float(sum(ngram_counter.values()))\n",
    "        self.adjusted_counts = self.calculate_adjusted_counts()\n",
    "\n",
    "    def count_of_counts(self):\n",
    "        count_of_counts_dict = {}\n",
    "        for count in self.ngram_counter.values():\n",
    "            count_of_counts_dict[count] = count_of_counts_dict.get(count, 0) + 1\n",
    "        return count_of_counts_dict\n",
    "\n",
    "    def calculate_adjusted_counts(self):\n",
    "        adjusted_counts = {}\n",
    "        count_of_counts_dict = self.count_of_counts()\n",
    "\n",
    "        for count in self.ngram_counter.values():\n",
    "            Nc_plus_1 = count_of_counts_dict.get(count + 1, 1)\n",
    "            Nc = count_of_counts_dict.get(count, 1)\n",
    "            c_star = (count + 1) * (Nc_plus_1 / Nc) \n",
    "            adjusted_counts[count] = c_star\n",
    "\n",
    "        return adjusted_counts\n",
    "\n",
    "    def smooth_ngram_probs(self):\n",
    "        smoothed_ngram_probs = {}\n",
    "        \n",
    "        for ngram, count in self.ngram_counter.items():\n",
    "            c_star = self.adjusted_counts.get(count, 1)\n",
    "            probability = c_star / self.total_ngrams\n",
    "            smoothed_ngram_probs[ngram] = probability\n",
    "\n",
    "        # Assigning the probability for unseen N-grams\n",
    "        unseen_ngram_prob = 1 / self.total_ngrams\n",
    "        smoothed_ngram_probs[None] = unseen_ngram_prob\n",
    "\n",
    "        return smoothed_ngram_probs\n",
    "    \n",
    "    def get_probability(self,trigram):\n",
    "        prob = 0;\n",
    "        return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ab2366fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearInterpolationSmoothing:\n",
    "    def __init__(self, trigram_counts, bigram_counts, unigram_counts, is_train=False, lambdas=None):\n",
    "        self.trigram_counts = trigram_counts\n",
    "        self.bigram_counts = bigram_counts\n",
    "        self.unigram_counts = unigram_counts\n",
    "        total_trigrams = 0\n",
    "        total_bigrams = 0\n",
    "        total_unigrams = 0\n",
    "        self.is_train = is_train\n",
    "        self.lambdas = lambdas\n",
    "\n",
    "    def update_lambdas(self):\n",
    "        updated_lambdas = {f'lambda{i}': self.lambdas[f'lambda{i}'] for i in range(1, 4)}\n",
    "        total_unigram = float(sum(self.unigram_counts.values()))\n",
    "\n",
    "        for trigram, trigram_count in self.trigram_counts.items():\n",
    "            t1, t2, t3 = trigram.split()\n",
    "\n",
    "            # Check if the frequency of the trigram is greater than 0\n",
    "            if trigram_count > 0:\n",
    "                # Calculate the three conditions\n",
    "                condition1 = (trigram_count - 1) / max(1, self.bigram_counts.get(f'{t1} {t2}', 0) - 1)\n",
    "                condition2 = (self.bigram_counts.get(f'{t2} {t3}', 0) - 1) / max(1, self.unigram_counts.get(t2, 0) - 1)\n",
    "                condition3 = (self.unigram_counts.get(t3, 0) - 1) / max(1, total_unigram - 1)\n",
    "\n",
    "                # Find the index of the maximum condition\n",
    "                max_condition_index = max(enumerate([condition1, condition2, condition3]), key=lambda x: x[1])[0]\n",
    "\n",
    "                # Update lambdas based on the maximum condition\n",
    "                updated_lambdas[f'lambda{max_condition_index + 1}'] += trigram_count\n",
    "\n",
    "        # Normalize the lambdas to ensure they sum to 1\n",
    "        lambda_sum = sum(updated_lambdas.values())\n",
    "        normalized_lambdas = {key: value / lambda_sum for key, value in updated_lambdas.items()}\n",
    "\n",
    "        return normalized_lambdas\n",
    "\n",
    "    def linear_interpolation_smoothing(self):\n",
    "        self.total_trigrams = sum(self.trigram_counts.values())\n",
    "        self.total_bigrams = sum(self.bigram_counts.values())\n",
    "        self.total_unigrams = sum(self.unigram_counts.values())\n",
    "\n",
    "        interpolated_probs = {}\n",
    "\n",
    "        if self.is_train:\n",
    "            self.lambdas = {f'lambda{i}': 0.0 for i in range(1, 4)}\n",
    "            self.lambdas = self.update_lambdas()\n",
    "\n",
    "        for trigram, count in self.trigram_counts.items():\n",
    "            t1, t2, t3 = trigram.split()\n",
    "\n",
    "            # Unigram probability\n",
    "            P1_t3 = self.unigram_counts.get(t3, 0) / self.total_unigrams\n",
    "\n",
    "            # Bigram probability\n",
    "            P2_t3 = self.bigram_counts.get(f\"{t2} {t3}\", 0) / self.total_bigrams\n",
    "\n",
    "            # Trigram probability\n",
    "            P3_t3 = count / self.total_trigrams\n",
    "\n",
    "            # Interpolation weights\n",
    "            lambda1 = self.lambdas.get('lambda1', 1/3.0)\n",
    "            lambda2 = self.lambdas.get('lambda2', 1/3.0)\n",
    "            lambda3 = self.lambdas.get('lambda3', 1/3.0)\n",
    "\n",
    "            # Linear interpolation\n",
    "            interpolated_prob = lambda1 * P1_t3 + lambda2 * P2_t3 + lambda3 * P3_t3\n",
    "\n",
    "            interpolated_probs[trigram] = interpolated_prob\n",
    "\n",
    "        if self.is_train:\n",
    "            return interpolated_probs, self.lambdas\n",
    "        else:\n",
    "            return interpolated_probs\n",
    "        \n",
    "    def get_probability(self,trigram):\n",
    "#         print(trigram)\n",
    "        t1, t2, t3 = trigram.split()\n",
    "\n",
    "        # Unigram probability\n",
    "        P1_t3 = self.unigram_counts.get(t3, 0) / self.total_unigrams\n",
    "\n",
    "        # Bigram probability\n",
    "        P2_t3 = self.bigram_counts.get(f\"{t2} {t3}\", 0) / self.total_bigrams\n",
    "\n",
    "        # Trigram probability\n",
    "        P3_t3 = self.trigram_counts.get(trigram,0) / self.total_trigrams\n",
    "#         print(P1_t3,P2_t3,P3_t3)\n",
    "        # Interpolation weights\n",
    "        lambda1 = self.lambdas.get('lambda1', 1/3.0)\n",
    "        lambda2 = self.lambdas.get('lambda2', 1/3.0)\n",
    "        lambda3 = self.lambdas.get('lambda3', 1/3.0)\n",
    "#         print(lambda1,lambda2, lambda3)\n",
    "        # Linear interpolation\n",
    "        interpolated_prob = lambda1 * P1_t3 + lambda2 * P2_t3 + lambda3 * P3_t3\n",
    "        if(interpolated_prob == 0):\n",
    "            interpolated_prob = 0.0001\n",
    "        return interpolated_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "487e2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage during training:\n",
    "# trigram_counts_train = {'a b c': 5, 'b c d': 3, 'c d e': 2}\n",
    "# bigram_counts_train = {'a b': 10, 'b c': 8, 'c d': 6}\n",
    "# unigram_counts_train = {'a': 15, 'b': 12, 'c': 9, 'd': 7, 'e': 5}\n",
    "\n",
    "# # Create an instance of LinearInterpolationSmoother during training\n",
    "# smoother_train = LinearInterpolationSmoother(trigram_counts_train, bigram_counts_train, unigram_counts_train, is_train=True)\n",
    "# interpolated_probs_train, updated_lambdas = smoother_train.linear_interpolation_smoothing()\n",
    "\n",
    "# # Example usage during testing with updated lambdas:\n",
    "# trigram_counts_test = {'x y z': 4, 'y z w': 2, 'z w v': 1}\n",
    "# bigram_counts_test = {'x y': 8, 'y z': 6, 'z w': 4}\n",
    "# unigram_counts_test = {'x': 10, 'y': 8, 'z': 6, 'w': 4, 'v': 2}\n",
    "\n",
    "# # Create an instance of LinearInterpolationSmoother during testing and pass updated lambdas\n",
    "# smoother_test = LinearInterpolationSmoother(trigram_counts_test, bigram_counts_test, unigram_counts_test, is_train=False, lambdas=updated_lambdas)\n",
    "# interpolated_probs_test = smoother_test.linear_interpolation_smoothing()\n",
    "\n",
    "# # Display the interpolated probabilities during training\n",
    "# print(\"Training:\")\n",
    "# for trigram, prob in interpolated_probs_train.items():\n",
    "#     print(f\"{trigram}: {prob}\")\n",
    "\n",
    "# # Display the interpolated probabilities during testing\n",
    "# print(\"\\nTesting:\")\n",
    "# for trigram, prob in interpolated_probs_test.items():\n",
    "#     print(f\"{trigram}: {prob}\")\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"This is a mohit sharma. han han a MOHIT Sharma.\"\n",
    "# tokenizer = TextTokenizer()  # Assuming TextTokenizer is your tokenizer class\n",
    "# nGramContext, nGramCounter = BuildNGram(n=3, txt=text, tokenizer=tokenizer)\n",
    "\n",
    "# # Print results\n",
    "# print(\"N-Gram Context:\", nGramContext)\n",
    "# print(\"N-Gram Counter:\", nGramCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "90a8928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_model(N, corpus_path,tokenizer):\n",
    "\n",
    "    # Read the corpus from the file\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "\n",
    "    # Generate N-grams using the BuildNGram function\n",
    "    nGramContext, nGramCounter = BuildNGram(N, corpus,tokenizer)\n",
    "\n",
    "    return nGramContext, nGramCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "51105f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model:\n",
    "    def __init__(self, corpus_path, result_name=\"2022201060_LM1\", n_gram_order=3, smoothing_type=\"g\", lambdas=None):\n",
    "        self.n_gram_order = n_gram_order\n",
    "        self.smoothing_type = smoothing_type\n",
    "        self.lambdas = lambdas\n",
    "        self.corpus_path = corpus_path\n",
    "        self.save_file_path = self._get_save_file_path(0)\n",
    "        self.train_corpus_path = self._get_save_file_path(1)\n",
    "        self.test_corpus_path = self._get_save_file_path(2)\n",
    "        self.test_samples_count = 1000\n",
    "        self.result_name = result_name\n",
    "        self.nGramContext = None\n",
    "        self.nGramCounter = None\n",
    "        self.probs = None\n",
    "        self.smoothing_instance = None  # Instance to hold the smoothing object\n",
    "        self.tokenizer = TextTokenizer()  # Instance to hold the TextTokenizer object\n",
    "\n",
    "    def _get_save_file_path(self, file_type):\n",
    "        # Extract the base name of the corpus path\n",
    "        corpus_name = os.path.basename(self.corpus_path) if self.corpus_path else \"corpus_unknown.txt\"\n",
    "\n",
    "        if file_type == 0:\n",
    "            return f\"{self.n_gram_order}_{self.smoothing_type}_{corpus_name}.json\"\n",
    "        elif file_type == 1:\n",
    "            return f\"{self.n_gram_order}_{self.smoothing_type}_{corpus_name}_train.txt\"\n",
    "        elif file_type == 2:\n",
    "            return f\"{self.n_gram_order}_{self.smoothing_type}_{corpus_name}_test.txt\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file_type. Use 0 for save_file, 1 for train_corpus, and 2 for test_corpus.\")\n",
    "    def setup(self, corpus_path=None):\n",
    "        # If corpus_path is not provided, use the path stored in self.corpus_path\n",
    "        if corpus_path is None:\n",
    "            corpus_path = self.corpus_path\n",
    "\n",
    "        # Read the corpus from the file\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "            corpus = file.read()\n",
    "\n",
    "        # Split the corpus into sentences\n",
    "        sentences = self.tokenizer.split_sentences(corpus)\n",
    "        # Exclude sentences with zero words\n",
    "        sentences = [sentence for sentence in sentences if len(sentence.split()) > 0]\n",
    "#         for s in sentences:\n",
    "#             print(s)\n",
    "#             print(\"--------------------\")\n",
    "        # Randomly select 1000 sentences for testing\n",
    "        selected_sentences = random.sample(sentences, min(self.test_samples_count, len(sentences)))\n",
    "\n",
    "        # Write the selected sentences to the test corpus file\n",
    "        with open(self.test_corpus_path, 'w', encoding='utf-8') as test_file:\n",
    "            test_file.write(\"\\n\".join(selected_sentences))\n",
    "\n",
    "        # Write the remaining sentences to the train corpus file\n",
    "        remaining_sentences = [sentence for sentence in sentences if sentence not in selected_sentences]\n",
    "        with open(self.train_corpus_path, 'w', encoding='utf-8') as train_file:\n",
    "            train_file.write(\"\\n\".join(remaining_sentences))\n",
    "\n",
    "#         print(f\"Setup complete. Train corpus: {self.train_corpus_path}, Test corpus: {self.test_corpus_path}\")\n",
    "\n",
    "    def train(self, corpus_path = None):\n",
    "        if(corpus_path):\n",
    "            self.nGramContext, self.nGramCounter = generate_ngram_model(self.n_gram_order,corpus_path,self.tokenizer)\n",
    "        else:   \n",
    "            self.nGramContext, self.nGramCounter = generate_ngram_model(self.n_gram_order, self.corpus_path,self.tokenizer)\n",
    "        \n",
    "        if self.smoothing_type == \"g\":\n",
    "            smoothing_instance = GoodTuringSmoothing(self.nGramCounter)\n",
    "            self.probs = smoothing_instance.smooth_ngram_probs()\n",
    "        elif self.smoothing_type == \"i\":\n",
    "            if(corpus_path):\n",
    "                bigram_context, bigram_counter = generate_ngram_model(2, corpus_path, self.tokenizer)\n",
    "                unigram_context, unigram_counter = generate_ngram_model(1, corpus_path, self.tokenizer)\n",
    "            else:    \n",
    "                bigram_context, bigram_counter = generate_ngram_model(2, self.corpus_path,self.tokenizer)\n",
    "                unigram_context, unigram_counter = generate_ngram_model(1, self.corpus_path,self.tokenizer)\n",
    "            smoothing_instance = LinearInterpolationSmoothing(self.nGramCounter, bigram_counter, unigram_counter, is_train=True, lambdas=self.lambdas)\n",
    "            self.probs, self.lambdas = smoothing_instance.linear_interpolation_smoothing()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid smoothing type. Choose 'g' for Good-Turing or 'i' for Linear Interpolation.\")\n",
    "\n",
    "        # Store the smoothing instance for future reference\n",
    "        self.smoothing_instance = smoothing_instance\n",
    "\n",
    "    def save(self):\n",
    "        # Save all necessary variables to a JSON file\n",
    "        model_state = {\n",
    "            'n_gram_order': self.n_gram_order,\n",
    "            'probs': self.probs,\n",
    "            'lambdas': self.lambdas,\n",
    "            'nGramContext': self.nGramContext,\n",
    "            'nGramCounter': self.nGramCounter,\n",
    "        }\n",
    "\n",
    "        with open(self.save_file_path, 'w') as file:\n",
    "            json.dump(model_state, file)\n",
    "\n",
    "    def load(self):\n",
    "        # Load all necessary variables from a JSON file\n",
    "        with open(self.save_file_path, 'r') as file:\n",
    "            model_state = json.load(file, object_hook=self.json_object_hook)\n",
    "\n",
    "        self.n_gram_order = model_state['n_gram_order']\n",
    "        self.probs = model_state['probs']\n",
    "        self.lambdas = model_state['lambdas']\n",
    "        self.nGramContext = model_state['nGramContext']\n",
    "        self.nGramCounter = model_state['nGramCounter']\n",
    "\n",
    "    def json_object_hook(self, dct):\n",
    "        # Replace the key 'null' with None during JSON deserialization\n",
    "        return {key if key != 'null' else None: value for key, value in dct.items()}\n",
    "    \n",
    "    def calculate_probability(self, sentence):\n",
    "        probability = 1.0\n",
    "        tokenized_sentence = self.tokenizer.sentence_tokenizer(sentence)\n",
    "        if( n_gram_order - 2 > 0 ):\n",
    "            tokenized_sentence = [\"<sos>\"] * (n_gram_order - 2) + tokenized_sentence\n",
    "        print(tokenized_sentence)\n",
    "        total_words = len(tokenized_sentence) - 2\n",
    "        for i in range(self.n_gram_order - 1, len(tokenized_sentence)):\n",
    "            context = \" \".join(tokenized_sentence[max(0, i - self.n_gram_order + 1): i ])\n",
    "            target_word = tokenized_sentence[i]\n",
    "            n_gram = f\"{context} {target_word}\" if context else target_word\n",
    "\n",
    "            # Calculate log likelihood based on the model probabilities\n",
    "#             print(n_gram)\n",
    "            prob = self.smoothing_instance.get_probability(n_gram)\n",
    "            print(n_gram, prob)\n",
    "            probability*=prob\n",
    "        p ,_ = self.perplexity(tokenized_sentence)\n",
    "        print(p)\n",
    "        return probability\n",
    "    \n",
    "    def perplexity(self, tokenized_sentence):\n",
    "        total_words = len(tokenized_sentence) - 2  # Exclude <SOS> and <EOS>\n",
    "        log_likelihood_sentence = 0.0\n",
    "#         print(tokenized_sentence)\n",
    "        for i in range(self.n_gram_order - 1, len(tokenized_sentence)):\n",
    "            context = \" \".join(tokenized_sentence[max(0, i - self.n_gram_order + 1): i ])\n",
    "            target_word = tokenized_sentence[i]\n",
    "            n_gram = f\"{context} {target_word}\" if context else target_word\n",
    "\n",
    "            # Calculate log likelihood based on the model probabilities\n",
    "#             print(n_gram)\n",
    "            prob = self.smoothing_instance.get_probability(n_gram)\n",
    "#             print(prob)\n",
    "            log_likelihood_sentence += math.log(prob)\n",
    "\n",
    "        perplexity_sentence = 2 ** (-log_likelihood_sentence / total_words)\n",
    "        return perplexity_sentence, log_likelihood_sentence\n",
    "\n",
    "    def evaluate_helper(self,result_file_path, corpus_path):\n",
    "        # Read the corpus from the file\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as file:\n",
    "            corpus = file.read()\n",
    "\n",
    "        sentences = self.tokenizer.split_sentences(corpus)\n",
    "        # Exclude sentences with zero words\n",
    "#         sentences = [sentence for sentence in sentences if len(sentence.split()) > 0]\n",
    "        \n",
    "#         print(sentences)\n",
    "        total_words = 0\n",
    "        log_likelihood_sum = 0.0\n",
    "        perplexity_scores = []\n",
    "        \n",
    "        with open(result_file_path, 'w', encoding='utf-8') as result_file:\n",
    "            # Initialize average perplexity\n",
    "            average_perplexity = None\n",
    "\n",
    "            for sentence in sentences:\n",
    "                tokenized_sentence = self.tokenizer.sentence_tokenizer(sentence)\n",
    "                if len(tokenized_sentence) > self.n_gram_order:\n",
    "                    perplexity_sentence, log_likelihood_sentence = self.perplexity(tokenized_sentence)\n",
    "                    perplexity_scores.append(perplexity_sentence)\n",
    "    #                 print(sentence , \" : perplexity  score : \",perplexity_sentence )\n",
    "\n",
    "                    # Write results to the file\n",
    "                    result_file.write(f\"{sentence}\\t{perplexity_sentence}\\n\")\n",
    "\n",
    "                    log_likelihood_sum += log_likelihood_sentence\n",
    "                    total_words += len(sentence.split())\n",
    "\n",
    "            if total_words >0:\n",
    "            # Calculate average perplexity\n",
    "                average_perplexity = 2 ** (-log_likelihood_sum / total_words)\n",
    "\n",
    "            # Move the cursor to the beginning of the file\n",
    "            result_file.seek(0)\n",
    "\n",
    "            # Write average perplexity at the first line of the file\n",
    "            result_file.write(f\"avg_perplexity\\t{average_perplexity}\\n\")\n",
    "        return average_perplexity\n",
    "        \n",
    "    def evaluate(self, train=True, test=True):\n",
    "        if train:\n",
    "            corpus_path = self.train_corpus_path\n",
    "            result_file_path = f\"{self.result_name}_train-perplexity.txt\"\n",
    "            avg_perp = self.evaluate_helper(result_file_path,corpus_path)\n",
    "            print(\"Average Perplexity on Train Set:\",avg_perp)\n",
    "        if test:\n",
    "            corpus_path = self.test_corpus_path\n",
    "            result_file_path = f\"{self.result_name}_test-perplexity.txt\"\n",
    "            avg_perp = self.evaluate_helper(result_file_path,corpus_path)\n",
    "            print(\"Average Perplexity on Test Set:\",avg_perp)\n",
    "        if (not train and not test ):\n",
    "            raise ValueError(\"Either train or test flag should be True.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a042bb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity on Train Set: 321.30966537090006\n",
      "Average Perplexity on Test Set: 310.29411892895706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "# corpus_path = \"sample_corpus.txt\"\n",
    "# corpus_path = \"./corpus/Pride and Prejudice - Jane Austen.txt\"\n",
    "corpus_path = \"./corpus/Ulysses  James Joyce.txt\"\n",
    "result_name = \"2022201060_LM1\"\n",
    "n_gram_order = 3\n",
    "smoothing_type = \"i\"\n",
    "lambdas = None\n",
    "\n",
    "# Create an instance of the Model class\n",
    "lm_model = Model(corpus_path,result_name, smoothing_type=smoothing_type)\n",
    "\n",
    "# Setup and train the model\n",
    "lm_model.setup()\n",
    "# print(\"<------------------------------ || setup ||------------------------------------->\")\n",
    "# lm_model.train(corpus_path)\n",
    "lm_model.train()\n",
    "# print(\"<------------------------------ || train ||------------------------------------->\")\n",
    "\n",
    "# Save the model\n",
    "# lm_model.save()\n",
    "\n",
    "# Load the model\n",
    "# lm_model.load()\n",
    "sentence = \"This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever.\"\n",
    "# lm_model.calculate_probability(sentence)\n",
    "# Evaluate on the test set\n",
    "lm_model.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22354074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for user input\n",
    "input_sentence =\"This eBook is for the use of\"\n",
    "\n",
    "# Get the last 2 word in the input sentence as the prefix\n",
    "prefix = input_sentence.split()[-1] if input_sentence else \"\"\n",
    "\n",
    "# Generate k candidates for the next word\n",
    "candidates = lm_model.generate_candidates(prefix, k)\n",
    "\n",
    "# Print the output\n",
    "print(\"output:\")\n",
    "for candidate, probability in candidates:\n",
    "    print(f\"{candidate} {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b15c69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
